Fino a questo momento trattando il tema della memoria virtuale abbiamo assunto che le diverse parti della memoria centrale siano uguali, o almeno che vi si potesse accedere nello stesso modo. Su sistemi con accesso non uniforme alla memoria (numa) dotati di più cpu (Paragrafo 1.3.2) non è così. Su questi sistemi una determinata cpu può accedere ad alcune sezioni della memoria principale più velocemente di quanto possa accedere ad altre. Queste differenze di prestazioni sono causate dal modo in cui cpu e memoria sono interconnesse all’interno del sistema. 
Un sistema di questo tipo è costituito da più cpu, ciascuna con la propria memoria locale 
![[Pasted image 20221214173032.png]]
le cpu sono organizzate utilizzando un’interconnessione di sistema condivisa e, come ci si potrebbe aspettare, una cpu può accedere più velocemente alla propria memoria locale rispetto alla memoria locale di un’altra cpu. I sistemi numa sono, senza eccezioni, più lenti dei sistemi in cui tutti gli accessi alla memoria principale sono trattati allo stesso modo. Tuttavia, come descritto nel Paragrafo 1.3.2, questi sistemi possono ospitare diverse cpu e raggiungere quindi livelli maggiori di throughput e parallelismo.

Le decisioni su quali frame di pagina memorizzare in quale posizione possono condizionare in modo significativo le prestazioni nei sistemi numa.
Se, in sistemi del genere, consideriamo uniforme la memoria, i processori potrebbero dover aspettare molto più a lungo per accedere alla memoria rispetto al caso in cui gli algoritmi di allocazione della memoria siano modificati per tenere in conto il numa.
Il loro obiettivo è quello di disporre di frame di memoria allocati “il più vicino possibile” alla cpu su cui è in esecuzione il processo (per “vicino” intendiamo “con latenza minima”, ovvero, di solito, sulla stessa scheda della cpu). 
Pertanto, quando un processo incorre in un page fault, un sistema di memoria virtuale conscio del numa assegna a quel processo un frame il più vicino possibile alla cpu su cui è in esecuzione. Per tenere in considerazione numa, lo scheduler deve tenere traccia dell’ultima cpu su cui è stato eseguito ciascun processo. Se lo scheduler tenta di schedulare ciascun processo sulla cpu precedente e il sistema di gestione della memoria virtuale tenta di allocare i frame per il processo vicino alla cpu su cui il processo viene schedulato, si otterrà un incremento di cache hit e una diminuzione del tempo di accesso alla memoria.

La questione diventa ancora più complicata con l’aggiunta dei thread. Per esempio, un processo con molti thread in esecuzione potrebbe vedere quei thread schedulati su differenti schede del sistema. Come viene allocata la memoria in questo caso?

Come discusso nel Paragrafo 5.7.1 Linux gestisce questa situazione facendo in modo che il kernel identifichi una gerarchia di domini di scheduling. Lo scheduler cfs di Linux non consente ai thread di migrare su domini diversi e quindi incorrere in penalità nell’accesso alla memoria. Linux, inoltre, ha una lista dei frame liberi distinta per ogni nodo numa, garantendo in tal modo che a un thread sia allocata memoria dal nodo su cui è in esecuzione. Solaris risolve il problema in modo simile creando le entità lgroup (“gruppi di località”) nel kernel. Ogni lgroup raccoglie alcune cpu e memoria e ogni cpu del gruppo può accedere alla memoria del gruppo entro un intervallo di latenza definito. Inoltre, esiste una gerarchia di lgroup basata sulla latenza tra i gruppi, simile alla gerarchia dei domini di scheduling in Linux. Solaris tenta di pianificare tutti i thread di un processo e di allocare tutta la memoria di un processo nell’ambito di un solo lgroup. Se una tale soluzione non è possibile, per il resto delle risorse necessarie vengono utilizzati gli lgroup più vicini, in modo da minimizzare la latenza complessiva della memoria e massimizzare il tasso di successo della cache del processore.